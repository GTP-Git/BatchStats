

# BatchStats.py
# C
# 14JUN25
# Added quadrant analysis statistics
# Schema updated 07JUN25


'''
###############################################################
###############################################################


--- Schema Start ---
'sgs_version': (str): "1.0"
'player_names': (list, 2 items):
(str): "AI 1"
(str): "AI 2"
'sgs_initial_bag': (list, 100 items):
(str): "D"
(str): "X"
(str): "O"
    ... (and 97 more items)
'initial_racks_sgs': (list, 2 items):
(list, 7 items):
(str): "L"
(str): "A"
(str): "I"
      ... (and 4 more items)
(list, 7 items):
(str): "R"
(str): "W"
(str): "D"
      ... (and 4 more items)
'full_move_history': (list, 24 items):
    'player': (int): 1
    'move_type': (str): "place"
    'rack': (list, 7 items):
(str): "L"
(str): "A"
(str): "I"
        ... (and 4 more items)
    'score': (int): 22
    'word': (str): "JILL"
    'positions': (list, 4 items):
(tuple, 3 items):
(int): 7
(int): 7
(str): "J"
(tuple, 3 items):
(int): 7
(int): 8
(str): "I"
(tuple, 3 items):
(int): 7
(int): 9
(str): "L"
        ... (and 1 more items)
    'blanks': (set): set() (empty)
    'drawn': (list, 4 items):
(str): "T"
(str): "A"
(str): "V"
        ... (and 1 more items)
    'coord': (str): "8H"
    'word_with_blanks': (str): "JILL"
    'is_bingo': (bool): False
    'turn_duration': (float): 0.0
    'total_expected_draw_value': (float): 2.9681374511827956
    'luck_factor': (float): -14.275417451182797
    'tiles_played_from_rack': (list, 4 items):
(str): "J"
(str): "I"
(str): "L"
        ... (and 1 more items)
    'leave': (list, 3 items):
(str): "L"
(str): "A"
(str): "N"
    'newly_placed': (list, 4 items):
(tuple, 3 items):
(int): 7
(int): 7
(str): "J"
(tuple, 3 items):
(int): 7
(int): 8
(str): "I"
(tuple, 3 items):
(int): 7
(int): 9
(str): "L"
        ... (and 1 more items)
    'start': (tuple, 2 items):
(int): 7
(int): 7
    'direction': (str): "H"
    'player': (int): 2
    'move_type': (str): "place"
    'rack': (list, 7 items):
(str): "R"
(str): "W"
(str): "D"
        ... (and 4 more items)
    'score': (float): 16.0
    'word': (str): "WILD"
    'positions': (list, 4 items):
(tuple, 3 items):
(int): 5
(int): 9
(str): "W"
(tuple, 3 items):
(int): 6
(int): 9
(str): "I"
(tuple, 3 items):
(int): 7
(int): 9
(str): "L"
        ... (and 1 more items)
    'blanks': (set): set() (empty)
    'drawn': (list, 3 items):
(str): "Z"
(str): "A"
(str): "A"
    'coord': (str): "J6"
    'word_with_blanks': (str): "WILD"
    'is_bingo': (bool): False
    'turn_duration': (float): 0.0
    'total_expected_draw_value': (float): 1.5162524514606743
    'luck_factor': (float): 2.7105889485393253
    'tiles_played_from_rack': (list, 3 items):
(str): "D"
(str): "I"
(str): "W"
    'leave': (list, 4 items):
(str): "R"
(str): "S"
(str): " "
        ... (and 1 more items)
    'newly_placed': (list, 3 items):
(tuple, 3 items):
(int): 8
(int): 9
(str): "D"
(tuple, 3 items):
(int): 6
(int): 9
(str): "I"
(tuple, 3 items):
(int): 5
(int): 9
(str): "W"
    'start': (tuple, 2 items):
(int): 5
(int): 9
    'direction': (str): "V"
    'player': (int): 1
    'move_type': (str): "place"
    'rack': (list, 7 items):
(str): "A"
(str): "N"
(str): "L"
        ... (and 4 more items)
    'score': (float): 24.0
    'word': (str): "VALIANT"
    'positions': (list, 7 items):
(tuple, 3 items):
(int): 4
(int): 8
(str): "V"
(tuple, 3 items):
(int): 5
(int): 8
(str): "A"
(tuple, 3 items):
(int): 6
(int): 8
(str): "L"
        ... (and 4 more items)
    'blanks': (set): set() (empty)
    'drawn': (list, 6 items):
(str): "O"
(str): "H"
(str): "V"
        ... (and 3 more items)
    'coord': (str): "I5"
    'word_with_blanks': (str): "VALIANT"
    'is_bingo': (bool): False
    'turn_duration': (float): 0.0
    'total_expected_draw_value': (float): 5.275505839534884
    'luck_factor': (float): 12.109374160465116
    'tiles_played_from_rack': (list, 6 items):
(str): "A"
(str): "L"
(str): "A"
        ... (and 3 more items)
    'leave': (list, 1 items):
(str): "T"
    'newly_placed': (list, 6 items):
(tuple, 3 items):
(int): 8
(int): 8
(str): "A"
(tuple, 3 items):
(int): 6
(int): 8
(str): "L"
(tuple, 3 items):
(int): 5
(int): 8
(str): "A"
        ... (and 3 more items)
    'start': (tuple, 2 items):
(int): 4
(int): 8
    'direction': (str): "V"
    ... (and 21 more items)
'final_scores_adjusted': (list, 2 items):
(float): 373.0
(float): 439.0
'game_mode_info':   'game_mode_str': (str): "AI vs AI"
  'practice_mode_str': (NoneType): None
'game_settings':   'use_endgame_solver': (bool): False
  'use_ai_simulation': (bool): False
  'is_ai_config': (list, 2 items):
(bool): True
(bool): True
  'letter_checks': (list, 4 items):
(bool): True
(bool): True
(bool): True
      ... (and 1 more items)
  'number_checks': (list, 6 items):
(bool): True
(bool): True
(bool): True
      ... (and 3 more items)
  'ai_simulation_parameters':     'num_candidates': (int): 10
    'num_opponent_sims': (int): 50
    'num_post_sim_candidates': (int): 10
  'bbb_7l_max_prob': (int): 1000
  'bbb_8l_max_prob': (int): 1000
--- Schema End ---


###############################################################
###############################################################
'''


import tkinter as tk
from tkinter import filedialog, scrolledtext
import os
import pickle
import csv
import pandas as pd # For statistical analysis
import matplotlib.pyplot as plt
import subprocess


# =================================================================================================
# /////////////////////////////////////////////////////////////////////////////////////////////////
# --- CONFIGURATION & CONSTANTS ---
# /////////////////////////////////////////////////////////////////////////////////////////////////
# =================================================================================================

SGS_FILES_DIRECTORY = "/Users/gregmacbook/Documents/Batch Games"
OUTPUT_CSV_FILE = "batch_stats_summary.csv"
BINGO_MIN_TILES = 7 # Minimum number of tiles played to be considered a bingo

# --- Game Data Keys (based on observed .sgs file structure) ---
# These keys provide a single point of reference for the raw data structure.
# The ParsedGame class (Layer 2) will use these to abstract the data.
SGS_VERSION_KEY = 'sgs_version'
FULL_MOVE_HISTORY_KEY = 'full_move_history'
FINAL_SCORES_ADJUSTED_KEY = 'final_scores_adjusted'

MOVE_PLAYER_INDEX_KEY = 'player'
MOVE_SCORE_KEY = 'score'
MOVE_IS_BINGO_FLAG_KEY = 'is_bingo'
MOVE_TILES_PLAYED_KEY = 'tiles_played_from_rack'


# =================================================================================================
# /////////////////////////////////////////////////////////////////////////////////////////////////
# --- LAYER 1: DATA LOADING & PARSING (SGS Files -> Raw Game Object) ---
# /////////////////////////////////////////////////////////////////////////////////////////////////
# =================================================================================================

def load_sgs_file(filename, status_widget):
    """
    Loads a complete game state from an .sgs file using pickle.
    This is our primary Layer 1 component.
    """
    try:
        with open(filename, 'rb') as f_load:
            loaded_data = pickle.load(f_load)
        # Status update is now less critical here, but can be useful for debugging.
        # update_status(status_widget, f"Successfully loaded: {os.path.basename(filename)}")

        if not isinstance(loaded_data, dict):
            update_status(status_widget, f"Error: {os.path.basename(filename)} did not load as a dictionary.")
            return None
        return loaded_data
    except FileNotFoundError:
        update_status(status_widget, f"Error: File not found '{filename}'")
        return None
    except (pickle.UnpicklingError, EOFError) as e:
        update_status(status_widget, f"Error unpickling data from {os.path.basename(filename)}: {e}")
        return None
    except Exception as e:
        update_status(status_widget, f"Unexpected error loading {os.path.basename(filename)}: {e}")
        return None


# =================================================================================================
# /////////////////////////////////////////////////////////////////////////////////////////////////
# --- LAYER 2: GAME DATA ABSTRACTION (Raw Game Object -> Structured Game Object) ---
# /////////////////////////////////////////////////////////////////////////////////////////////////
# =================================================================================================

class ParsedGame:
    """
    A wrapper for the raw game_data dictionary loaded from an .sgs file.
    It provides a clean, stable interface for accessing game data, abstracting
    away the underlying dictionary keys.
    """
    def __init__(self, game_data, source_filename):
        self._data = game_data
        self._filename = os.path.basename(source_filename)

    def get_filename(self):
        """Returns the base filename of the source .sgs file."""
        return self._filename

    def is_valid(self):
        """
        Checks if the loaded game data contains the minimum required keys
        for basic statistical analysis.
        """
        if not self._data:
            return False
        
        final_scores = self._data.get(FINAL_SCORES_ADJUSTED_KEY)
        if final_scores is None or not isinstance(final_scores, list) or len(final_scores) < 2:
            print(f"Validation failed for {self._filename}: Missing or invalid '{FINAL_SCORES_ADJUSTED_KEY}'.")
            return False
            
        move_history = self._data.get(FULL_MOVE_HISTORY_KEY)
        if move_history is None or not isinstance(move_history, list):
            print(f"Validation failed for {self._filename}: Missing or invalid '{FULL_MOVE_HISTORY_KEY}'.")
            return False
            
        return True

    def get_final_scores(self):
        """Returns a list of final scores, e.g., [p1_score, p2_score]."""
        return self._data.get(FINAL_SCORES_ADJUSTED_KEY, [0, 0])

    def get_move_history(self):
        """Returns the full list of move dictionaries."""
        return self._data.get(FULL_MOVE_HISTORY_KEY, [])


# =================================================================================================
# /////////////////////////////////////////////////////////////////////////////////////////////////
# --- LAYER 3: STATISTIC CALCULATION ---
# /////////////////////////////////////////////////////////////////////////////////////////////////
# =================================================================================================

class BaseStatistic:
    """
    Abstract base class for a statistic calculator.
    Defines the interface that all concrete statistic classes must implement.
    """
    def get_column_names(self):
        """Returns a list of strings for the CSV header."""
        raise NotImplementedError("Subclasses must implement this method.")

    def calculate(self, parsed_game):
        """
        Takes a ParsedGame object and returns a dictionary of
        {column_name: value}.
        """
        raise NotImplementedError("Subclasses must implement this method.")

class GameIDStatistic(BaseStatistic):
    """Calculates the Game ID."""
    def get_column_names(self):
        return ["game_id"]

    def calculate(self, parsed_game):
        return {"game_id": parsed_game.get_filename()}

class PlayerScoreStatistic(BaseStatistic):
    """Calculates final scores for both players."""
    def get_column_names(self):
        return ["p1_score", "p2_score"]

    def calculate(self, parsed_game):
        scores = parsed_game.get_final_scores()
        return {"p1_score": scores[0], "p2_score": scores[1]}

class BingoCountStatistic(BaseStatistic):
    """Counts the number of bingos for each player."""
    def get_column_names(self):
        return ["p1_bingos", "p2_bingos"]

    def calculate(self, parsed_game):
        p1_bingos = 0
        p2_bingos = 0
        for move in parsed_game.get_move_history():
            if not isinstance(move, dict):
                continue

            is_bingo = move.get(MOVE_IS_BINGO_FLAG_KEY)
            player_idx = move.get(MOVE_PLAYER_INDEX_KEY)

            bingo_found = False
            if is_bingo is True:
                bingo_found = True
            elif is_bingo is None: # Fallback for older schemas
                tiles_played = move.get(MOVE_TILES_PLAYED_KEY, [])
                if isinstance(tiles_played, (list, str)) and len(tiles_played) >= BINGO_MIN_TILES:
                    bingo_found = True
            
            if bingo_found:
                if player_idx == 1:
                    p1_bingos += 1
                elif player_idx == 2:
                    p2_bingos += 1
        
        return {"p1_bingos": p1_bingos, "p2_bingos": p2_bingos}

class MaxTurnScoreStatistic(BaseStatistic):
    """Finds the single highest-scoring turn in the game."""
    def get_column_names(self):
        return ["game_max_turn_score"]

    def calculate(self, parsed_game):
        max_score = 0
        for move in parsed_game.get_move_history():
            if not isinstance(move, dict):
                continue
            
            turn_score = move.get(MOVE_SCORE_KEY, 0)
            if not isinstance(turn_score, (int, float)):
                turn_score = 0
            
            if turn_score > max_score:
                max_score = turn_score
        
        return {"game_max_turn_score": max_score}



#************ --- Board Structure Statistics ************# ---



class QuadrantUsageStatistic(BaseStatistic):
    """
    Calculates the number and percentage of tiles played in each quadrant.
    Uses an inclusive counting method based on the provided logic, where tiles
    on dividing lines (but not the center) are counted in adjacent quadrants.
    Q1=TopRight, Q2=TopLeft, Q3=BottomLeft, Q4=BottomRight.
    """
    def get_column_names(self):
        return [
            'q1_count', 'q2_count', 'q3_count', 'q4_count',
            'q1_perc', 'q2_perc', 'q3_perc', 'q4_perc'
        ]

    def calculate(self, parsed_game):
        counts = {"Q1": 0, "Q2": 0, "Q3": 0, "Q4": 0}
        center_r, center_c = 7, 7
        total_tiles_placed = 0

        for move in parsed_game.get_move_history():
            # The schema uses 'positions' for placed tiles in the move history.
            if move.get('move_type') != 'place':
                continue
            
            placed_tiles = move.get('newly_placed', [])
            if not isinstance(placed_tiles, list):
                continue

            for r, c, _ in placed_tiles:
                total_tiles_placed += 1

                # Check membership for each quadrant inclusively.
                # A tile on a dividing line will be counted in two quadrants.
                if r <= center_r and c >= center_c: counts["Q1"] += 1 # Top-Right
                if r <= center_r and c <= center_c: counts["Q2"] += 1 # Top-Left
                if r >= center_r and c <= center_c: counts["Q3"] += 1 # Bottom-Left
                if r >= center_r and c >= center_c: counts["Q4"] += 1 # Bottom-Right
        
        # Calculate percentages. The denominator is the total number of physical tiles placed.
        # Note: Because of the inclusive counting, the sum of these percentages may exceed 100.
        if total_tiles_placed > 0:
            q1_perc = (counts["Q1"] / total_tiles_placed) * 100.0
            q2_perc = (counts["Q2"] / total_tiles_placed) * 100.0
            q3_perc = (counts["Q3"] / total_tiles_placed) * 100.0
            q4_perc = (counts["Q4"] / total_tiles_placed) * 100.0
        else:
            q1_perc = q2_perc = q3_perc = q4_perc = 0.0

        return {
            'q1_count': counts["Q1"],
            'q2_count': counts["Q2"],
            'q3_count': counts["Q3"],
            'q4_count': counts["Q4"],
            'q1_perc': q1_perc,
            'q2_perc': q2_perc,
            'q3_perc': q3_perc,
            'q4_perc': q4_perc,
        }





class OpeningMoveStatistic(BaseStatistic):
    """
    Extracts the direction of the opening move of the game.
    """
    def get_column_names(self):
        return ['opening_direction']

    def calculate(self, parsed_game):
        move_history = parsed_game.get_move_history()
        
        # Find the first 'place' move in the history
        first_place_move = None
        for move in move_history:
            if move.get('move_type') == 'place':
                first_place_move = move
                break
        
        if first_place_move:
            direction = first_place_move.get('direction', 'N/A')
        else:
            direction = 'N/A' # Game may have had no moves
            
        return {'opening_direction': direction}


    




class SquareUsageStatistic(BaseStatistic):
    """
    A stateful statistic calculator that aggregates data across all games
    and saves its result to a separate file instead of the main CSV.
    It counts the number of times each square on the board is used.
    """
    def __init__(self):
        # 15x15 grid to store counts for each square
        self.square_counts = [[0] * 15 for _ in range(15)]
        self.games_processed = 0

    def get_column_names(self):
        # This statistic does not add columns to the main CSV.
        return []

    def calculate(self, parsed_game):
        # This method is repurposed to process a single game and update state.
        self.games_processed += 1
        for move in parsed_game.get_move_history():
            if move.get('move_type') != 'place':
                continue
            
            placed_tiles = move.get('newly_placed', [])
            if not isinstance(placed_tiles, list):
                continue

            for r, c, _ in placed_tiles:
                if 0 <= r < 15 and 0 <= c < 15:
                    self.square_counts[r][c] += 1
        
        # Return an empty dictionary as we are not adding to the CSV row.
        return {}

    def save_results(self, output_filename="square_usage_counts.json"):
        """Saves the aggregated counts and total games to a JSON file."""
        import json
        if self.games_processed == 0:
            print("Warning: No games were processed for SquareUsageStatistic.")
            return

        data_to_save = {
            "games_processed": self.games_processed,
            "square_counts": self.square_counts
        }
        with open(output_filename, 'w') as f:
            json.dump(data_to_save, f, indent=2)
        print(f"Square usage data saved to {output_filename}")






class ComebackAnalysisStatistic(BaseStatistic):
    """
    A stateful statistic that analyzes "comeback attempts".
    It checks turns where a player is trailing by 40+ points and categorizes
    their move as either opening a new quadrant or playing in existing ones.
    It then tracks the final win rate for each strategy.
    """
    def __init__(self):
        self.open_new_quad_attempts = 0
        self.open_new_quad_wins = 0
        self.play_existing_attempts = 0
        self.play_existing_wins = 0

    def get_column_names(self):
        return [] # Does not add to the main CSV

    def calculate(self, parsed_game):
        # --- Setup for a single game ---
        p1_score, p2_score = parsed_game.get_final_scores()
        p1_won_game = p1_score > p2_score
        
        p1_cumulative = 0
        p2_cumulative = 0
        active_quadrants = set()
        center_r, center_c = 7, 7

        # --- Reconstruct game turn-by-turn ---
        for move in parsed_game.get_move_history():
            if move.get('move_type') != 'place':
                continue

            player_idx = move.get('player')
            turn_score = move.get('score', 0)
            newly_placed = move.get('newly_placed', [])

            # --- Check for comeback attempt trigger ---
            is_comeback_attempt = False
            if player_idx == 1 and (p2_cumulative - p1_cumulative) >= 40:
                is_comeback_attempt = True
            elif player_idx == 2 and (p1_cumulative - p2_cumulative) >= 40:
                is_comeback_attempt = True

            if is_comeback_attempt:
                # --- Analyze the move ---
                move_quadrants = set()
                for r, c, _ in newly_placed:
                    # EXCLUSIVE counting for this analysis
                    if r < center_r and c > center_c: move_quadrants.add(1) # Top-Right
                    if r < center_r and c < center_c: move_quadrants.add(2) # Top-Left
                    if r > center_r and c < center_c: move_quadrants.add(3) # Bottom-Left
                    if r > center_r and c > center_c: move_quadrants.add(4) # Bottom-Right
                
                opened_new_quadrant = not move_quadrants.issubset(active_quadrants)
                
                if opened_new_quadrant:
                    self.open_new_quad_attempts += 1
                    if (player_idx == 1 and p1_won_game) or (player_idx == 2 and not p1_won_game):
                        self.open_new_quad_wins += 1
                else:
                    self.play_existing_attempts += 1
                    if (player_idx == 1 and p1_won_game) or (player_idx == 2 and not p1_won_game):
                        self.play_existing_wins += 1

            # --- Update state for next turn ---
            for r, c, _ in newly_placed:
                # EXCLUSIVE counting for this analysis
                if r < center_r and c > center_c: active_quadrants.add(1) # Top-Right
                if r < center_r and c < center_c: active_quadrants.add(2) # Top-Left
                if r > center_r and c < center_c: active_quadrants.add(3) # Bottom-Left
                if r > center_r and c > center_c: active_quadrants.add(4) # Bottom-Right

            if player_idx == 1:
                p1_cumulative += turn_score
            elif player_idx == 2:
                p2_cumulative += turn_score
        
        return {} # Return empty dict as we don't add to CSV

    def save_results(self, output_filename="comeback_analysis_results.json"):
        """Saves the aggregated analysis to a JSON file."""
        import json
        
        open_win_rate = (self.open_new_quad_wins / self.open_new_quad_attempts * 100) if self.open_new_quad_attempts > 0 else 0
        existing_win_rate = (self.play_existing_wins / self.play_existing_attempts * 100) if self.play_existing_attempts > 0 else 0

        results = {
            "strategy": ["Open New Quadrant", "Play in Existing Quadrants"],
            "attempts": [self.open_new_quad_attempts, self.play_existing_attempts],
            "wins": [self.open_new_quad_wins, self.play_existing_wins],
            "win_rate_percent": [open_win_rate, existing_win_rate]
        }
        with open(output_filename, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"Comeback analysis data saved to {output_filename}")


    



    

# --- Registry of all statistics to be calculated ---
STATISTICS_TO_RUN = [
    GameIDStatistic(),
    PlayerScoreStatistic(),
    BingoCountStatistic(),
    MaxTurnScoreStatistic(),
    QuadrantUsageStatistic(),
    OpeningMoveStatistic(),
]


# =================================================================================================
# /////////////////////////////////////////////////////////////////////////////////////////////////
# --- LAYER 4: CSV GENERATION & ORCHESTRATION ---
# /////////////////////////////////////////////////////////////////////////////////////////////////
# =================================================================================================

def process_all_sgs_files(directory, status_widget):
    """
    Orchestrates the entire process:
    1. Finds all .sgs files.
    2. For each file, creates a ParsedGame object.
    3. Runs all registered statistic calculators on it.
    4. Collects the results and writes them to a single CSV file.
    5. Saves results from any stateful statistics.
    """
    update_status(status_widget, f"Starting processing of SGS files in: {directory}")
    sgs_files = [f for f in os.listdir(directory) if f.endswith(".sgs")]

    if not sgs_files:
        update_status(status_widget, "No .sgs files found in the directory.")
        return None

    # Instantiate all statistics, including any stateful ones
    square_usage_stat = SquareUsageStatistic()
    comeback_stat = ComebackAnalysisStatistic()
    stats_to_run_for_csv = [
        GameIDStatistic(),
        PlayerScoreStatistic(),
        BingoCountStatistic(),
        MaxTurnScoreStatistic(),
        QuadrantUsageStatistic(),
        OpeningMoveStatistic(),
    ]
    all_stats_processors = stats_to_run_for_csv + [square_usage_stat]
    all_stats_processors.append(comeback_stat)

    all_games_data = []
    for sgs_file in sgs_files:
        filepath = os.path.join(directory, sgs_file)
        update_status(status_widget, f"Processing: {sgs_file}...")
        
        raw_game_data = load_sgs_file(filepath, status_widget)
        if not raw_game_data:
            update_status(status_widget, f"Failed to load or read {sgs_file}. Skipping.")
            continue

        parsed_game = ParsedGame(raw_game_data, sgs_file)
        if not parsed_game.is_valid():
            update_status(status_widget, f"Skipping {sgs_file} due to missing critical data.")
            continue

        # Calculate all stats for this game
        game_stats_row = {}
        for statistic_calculator in all_stats_processors:
            try:
                result = statistic_calculator.calculate(parsed_game)
                if result:
                    game_stats_row.update(result)
            except Exception as e:
                update_status(status_widget, f"Error calculating {type(statistic_calculator).__name__} for {sgs_file}: {e}")

        if game_stats_row:
            all_games_data.append(game_stats_row)

    # After processing all files, save results from stateful statistics
    square_usage_stat.save_results()
    comeback_stat.save_results()

    if not all_games_data:
        update_status(status_widget, "No data successfully extracted for CSV from any SGS files.")
        return False

    # Write to CSV
    try:
        fieldnames = []
        for stat in stats_to_run_for_csv:
            fieldnames.extend(stat.get_column_names())

        with open(OUTPUT_CSV_FILE, 'w', newline='') as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(all_games_data)
        update_status(status_widget, f"Successfully wrote data for {len(all_games_data)} games to {OUTPUT_CSV_FILE}")
        return True
    except IOError as e:
        update_status(status_widget, f"Error writing to CSV {OUTPUT_CSV_FILE}: {e}")
        return False
    except Exception as e:
        update_status(status_widget, f"Unexpected error writing CSV: {e}")
        return False


# =================================================================================================
# /////////////////////////////////////////////////////////////////////////////////////////////////
# --- LAYER 5: POST-CSV ANALYSIS (Pandas) ---
# /////////////////////////////////////////////////////////////////////////////////////////////////
# =================================================================================================

def calculate_and_display_stats(status_widget):
    """
    Reads the generated CSV using Pandas and calculates/displays statistics.
    """
    try:
        df = pd.read_csv(OUTPUT_CSV_FILE)
    except FileNotFoundError:
        update_status(status_widget, f"Error: CSV file {OUTPUT_CSV_FILE} not found. Cannot calculate stats.")
        return
    except pd.errors.EmptyDataError:
        update_status(status_widget, f"Error: CSV file {OUTPUT_CSV_FILE} is empty. Cannot calculate stats.")
        return
    except Exception as e:
        update_status(status_widget, f"Error reading CSV with Pandas {OUTPUT_CSV_FILE}: {e}")
        return

    if df.empty:
        update_status(status_widget, "CSV file is empty. No statistics to calculate.")
        return

    num_games = len(df)
    update_status(status_widget, f"\n--- Statistics based on {num_games} games ---")

    # Cumulative average scores
    avg_p1_score = df['p1_score'].mean()
    avg_p2_score = df['p2_score'].mean()
    update_status(status_widget, f"Average P1 Score: {avg_p1_score:.2f}")
    update_status(status_widget, f"Average P2 Score: {avg_p2_score:.2f}")

    # Average bingos per game
    avg_p1_bingos = df['p1_bingos'].mean()
    avg_p2_bingos = df['p2_bingos'].mean()
    update_status(status_widget, f"Average P1 Bingos per Game: {avg_p1_bingos:.2f}")
    update_status(status_widget, f"Average P2 Bingos per Game: {avg_p2_bingos:.2f}")

    # Average of the "largest single score per game"
    avg_max_turn_score = df['game_max_turn_score'].mean()
    update_status(status_widget, f"Average Max Turn Score per Game: {avg_max_turn_score:.2f}")

    update_status(status_widget, "\n--- End of Statistics ---")


# =================================================================================================
# /////////////////////////////////////////////////////////////////////////////////////////////////
# --- LAYER 6: USER INTERFACE (Tkinter) & UTILITIES ---
# /////////////////////////////////////////////////////////////////////////////////////////////////
# =================================================================================================

def update_status(text_widget, message):
    """Appends a message to the status text widget and scrolls to the end."""
    text_widget.insert(tk.END, message + "\n")
    text_widget.see(tk.END)
    text_widget.update_idletasks() # Ensure GUI updates immediately

def start_processing(status_widget):
    """Main function called when the 'Gather Stats' button is pressed."""
    status_widget.delete('1.0', tk.END) # Clear previous status messages
    update_status(status_widget, "Process started...")

    # Check if directory exists
    if not os.path.isdir(SGS_FILES_DIRECTORY):
        update_status(status_widget, f"Error: Directory not found - {SGS_FILES_DIRECTORY}")
        update_status(status_widget, "Processing aborted.")
        return

    csv_written_successfully = process_all_sgs_files(SGS_FILES_DIRECTORY, status_widget)

    if csv_written_successfully:
        calculate_and_display_stats(status_widget)
    else:
        update_status(status_widget, "CSV file was not generated or is empty. Cannot calculate statistics.")

    update_status(status_widget, "\nProcessing finished.")

def start_schema_dump(status_widget):
    """
    Handles the 'Get Schema' button click. Prompts user for an .sgs file
    and then calls dump_sgs_schema to print its structure to the console.
    """
    status_widget.insert(tk.END, "\nAttempting to dump schema...\n")
    status_widget.see(tk.END)
    
    sgs_filepath = filedialog.askopenfilename(
        title="Select an .SGS File to Analyze Schema",
        initialdir=SGS_FILES_DIRECTORY, # Start in the default batch games directory
        filetypes=(("SGS files", "*.sgs"), ("All files", "*.*"))
    )

    if sgs_filepath: # If a file was selected
        update_status(status_widget, f"Selected file for schema dump: {sgs_filepath}")
        update_status(status_widget, "Schema will be printed to the CONSOLE / TERMINAL.")
        # Call the schema dumper function (which prints to console)
        dump_sgs_schema(sgs_filepath) 
        update_status(status_widget, "Schema dump to console complete.")
    else:
        update_status(status_widget, "Schema dump cancelled (no file selected).")

def dump_sgs_schema(sgs_filepath):
    """
    Loads a single .sGS file and prints its data structure (schema)
    to the console in a human-readable format.

    Args:
        sgs_filepath (str): The full path to the .sgs file to analyze.
    """
    print(f"\n--- Analyzing schema for: {sgs_filepath} ---")
    try:
        with open(sgs_filepath, 'rb') as f_load:
            game_data = pickle.load(f_load)
    except FileNotFoundError:
        print(f"Error: File not found '{sgs_filepath}'")
        return
    except (pickle.UnpicklingError, EOFError) as e:
        print(f"Error unpickling data from {sgs_filepath}: {e}")
        return
    except Exception as e:
        print(f"Unexpected error loading {sgs_filepath}: {e}")
        return

    if not isinstance(game_data, dict):
        print(f"Error: Loaded data from {sgs_filepath} is not a dictionary (Type: {type(game_data)}). Cannot dump schema.")
        return

    print("--- Schema Start ---")
    _recursive_print_schema(game_data, indent_level=0)
    print("--- Schema End ---")

def _recursive_print_schema(data_item, indent_level=0, current_path=""):
    """
    Helper function to recursively print the schema of a data item.
    """
    indent = "  " * indent_level
    MAX_LIST_ITEMS_TO_SHOW = 3 # Show first few items of a list
    MAX_STRING_LENGTH_TO_SHOW = 70 # Truncate long strings

    if isinstance(data_item, dict):
        if not data_item:
            print(f"{indent}{current_path}(dict): {{}} (empty)")
            return
        # print(f"{indent}{current_path}(dict):") # Optional: print dict path before its items
        for key, value in data_item.items():
            new_path = f"{current_path}.{key}" if current_path else key
            print(f"{indent}'{key}': ", end="") # Print key, then let recursive call print type/value
            _recursive_print_schema(value, indent_level + 1, new_path)
    
    elif isinstance(data_item, list):
        if not data_item:
            print(f"(list): [] (empty)")
        else:
            print(f"(list, {len(data_item)} items):")
            # Show type of first item, or first few items
            for i, item in enumerate(data_item[:MAX_LIST_ITEMS_TO_SHOW]):
                item_path_display = f"{current_path}[{i}]" # For context if needed, but can be verbose
                # print(f"{indent}  [{i}]: ", end="") # Print index, then let recursive call print type/value
                _recursive_print_schema(item, indent_level + 1, item_path_display)
            if len(data_item) > MAX_LIST_ITEMS_TO_SHOW:
                print(f"{indent}  ... (and {len(data_item) - MAX_LIST_ITEMS_TO_SHOW} more items)")
    
    elif isinstance(data_item, tuple):
        if not data_item:
            print(f"(tuple): () (empty)")
        else:
            print(f"(tuple, {len(data_item)} items):")
            for i, item in enumerate(data_item[:MAX_LIST_ITEMS_TO_SHOW]):
                item_path_display = f"{current_path}[{i}]"
                _recursive_print_schema(item, indent_level + 1, item_path_display)
            if len(data_item) > MAX_LIST_ITEMS_TO_SHOW:
                print(f"{indent}  ... (and {len(data_item) - MAX_LIST_ITEMS_TO_SHOW} more items)")

    elif isinstance(data_item, set):
        if not data_item:
            print(f"(set): set() (empty)")
        else:
            # Convert set to list for consistent display of first few items
            temp_list = list(data_item)
            print(f"(set, {len(temp_list)} items): {{")
            for i, item in enumerate(temp_list[:MAX_LIST_ITEMS_TO_SHOW]):
                # Sets don't have a path for items, just show the item itself
                _recursive_print_schema(item, indent_level + 1, "") # No path for set items
            if len(temp_list) > MAX_LIST_ITEMS_TO_SHOW:
                print(f"{indent}  ... (and {len(temp_list) - MAX_LIST_ITEMS_TO_SHOW} more items)")
            print(f"{indent}}}")


    elif isinstance(data_item, str):
        if len(data_item) > MAX_STRING_LENGTH_TO_SHOW:
            print(f"(str): \"{data_item[:MAX_STRING_LENGTH_TO_SHOW]}...\" (truncated)")
        else:
            print(f"(str): \"{data_item}\"")
    
    elif isinstance(data_item, bool):
        print(f"(bool): {data_item}")
        
    elif isinstance(data_item, (int, float)):
        print(f"({type(data_item).__name__}): {data_item}")
        
    elif data_item is None:
        print(f"(NoneType): None")
        
    else:
        # For any other types, just print their type and a string representation
        print(f"({type(data_item).__name__}): {str(data_item)}")

def setup_gui():
    """Sets up and runs the Tkinter GUI."""
    root = tk.Tk()
    root.title("Scrabble Batch Stats Processor")

    main_frame = tk.Frame(root, padx=10, pady=10)
    main_frame.pack(fill=tk.BOTH, expand=True)

    # Directory Info Label
    dir_label = tk.Label(main_frame, text=f"Looking for .sgs files in: {SGS_FILES_DIRECTORY}")
    dir_label.pack(pady=(0, 10))
    
    output_csv_label = tk.Label(main_frame, text=f"Output CSV will be: {OUTPUT_CSV_FILE}")
    output_csv_label.pack(pady=(0,10))

    # Status Text Area
    status_text = scrolledtext.ScrolledText(main_frame, wrap=tk.WORD, height=20, width=80)
    status_text.pack(pady=(0, 10), fill=tk.BOTH, expand=True)

    # --- Buttons ---
    button_frame = tk.Frame(main_frame)
    button_frame.pack(fill=tk.X)

    gather_button = tk.Button(button_frame, text="Gather Stats",
                              command=lambda: start_processing(status_text))
    gather_button.pack(side=tk.LEFT, expand=True, fill=tk.X, padx=5)

    schema_button = tk.Button(button_frame, text="Get Schema from File",
                              command=lambda: start_schema_dump(status_text))
    schema_button.pack(side=tk.LEFT, expand=True, fill=tk.X, padx=5)

    report_button = tk.Button(button_frame, text="Generate Report",
                              command=lambda: start_report_generation(status_text))
    report_button.pack(side=tk.LEFT, expand=True, fill=tk.X, padx=5)
    
    update_status(status_text, "Welcome to the Scrabble Batch Stats Processor.")
    update_status(status_text, "Click 'Gather Stats' to process batch files.")
    update_status(status_text, "Click 'Get Schema from File' to analyze an .sgs file structure (output to console).")

    root.mainloop()




# =================================================================================================
# /////////////////////////////////////////////////////////////////////////////////////////////////
# --- LAYER 7: Report Generation ---
# /////////////////////////////////////////////////////////////////////////////////////////////////
# =================================================================================================



def generate_analysis_report(status_widget):
    """
    LAYER 7: REPORT GENERATION ORCHESTRATOR
    Generates a PDF report by calling helper functions for each analysis
    section and then populating a Markdown template.
    """
    update_status(status_widget, "\n--- Starting Report Generation ---")
    
    # --- 1. Load Main Data File ---
    try:
        df = pd.read_csv(OUTPUT_CSV_FILE)
        if df.empty:
            update_status(status_widget, "Error: CSV file is empty. Cannot generate report.")
            return
    except FileNotFoundError:
        update_status(status_widget, f"Error: CSV file {OUTPUT_CSV_FILE} not found. Run 'Gather Stats' first.")
        return

    # --- 2. Run Individual Analysis Sections ---
    template_data = {}

    quadrant_results = generate_quadrant_analysis(df, status_widget)
    if not quadrant_results: return
    template_data.update(quadrant_results)

    dominant_quad_results = generate_dominant_quadrant_analysis(df, status_widget)
    if not dominant_quad_results: return
    template_data.update(dominant_quad_results)

    opening_orientation_results = generate_opening_orientation_analysis(df, status_widget)
    if not opening_orientation_results: return
    template_data.update(opening_orientation_results)

    quad_control_results = generate_quadrant_control_analysis(df, status_widget)
    if not quad_control_results: return
    template_data.update(quad_control_results)

    comeback_results = generate_comeback_analysis(status_widget)
    if not comeback_results: return
    template_data.update(comeback_results)

    heatmap_results = generate_heatmap_analysis(status_widget)
    if not heatmap_results: return
    template_data.update(heatmap_results)

    # --- 3. Load Template and Populate ---
    template_filename = "report_template.md"
    update_status(status_widget, f"Loading template file: {template_filename}...")
    try:
        with open(template_filename, 'r') as f:
            template_content = f.read()
        
        report_md_content = template_content.format(**template_data)
        
        with open("report.md", "w") as f:
            f.write(report_md_content)

    except FileNotFoundError:
        update_status(status_widget, f"ERROR: Template file '{template_filename}' not found.")
        return
    except KeyError as e:
        update_status(status_widget, f"ERROR: A placeholder {e} in the template was not provided a value.")
        return

    # --- 4. Convert to PDF using Pandoc ---
    output_pdf_filename = "Scrabble_Analysis_Report.pdf"
    update_status(status_widget, f"Converting report to PDF: {output_pdf_filename}...")
    try:
        custom_env = os.environ.copy()
        tex_path = '/Library/TeX/texbin'
        homebrew_path = '/opt/homebrew/bin'
        custom_env['PATH'] = f"{homebrew_path}{os.pathsep}{tex_path}{os.pathsep}{custom_env.get('PATH', '')}"

        subprocess.run(
            [
                'pandoc', 'report.md',
                '--pdf-engine', 'xelatex',
                '-V', 'header-includes=\\usepackage{placeins}',
                '-o', output_pdf_filename
            ],
            check=True, capture_output=True, text=True, env=custom_env
        )
        update_status(status_widget, f"SUCCESS: Report saved as {output_pdf_filename}")
    except subprocess.CalledProcessError as e:
        update_status(status_widget, "ERROR: Pandoc failed to generate PDF.")
        update_status(status_widget, f"Pandoc/XeLaTeX Error: {e.stderr}")
    except Exception as e:
        update_status(status_widget, f"An unexpected error occurred during PDF generation: {e}")






def generate_quadrant_analysis(df, status_widget):
    """
    Generates the quadrant analysis table and chart.
    
    Args:
        df (pd.DataFrame): The main dataframe with all game stats.
        status_widget: The tkinter widget for status updates.

    Returns:
        A dictionary containing the markdown table and chart path for the template,
        or None if an error occurs.
    """
    update_status(status_widget, "Generating quadrant analysis...")
    try:
        quadrant_chart_filename = "quadrant_usage_chart.png"
        
        quadrant_cols = ['q1_perc', 'q2_perc', 'q3_perc', 'q4_perc']
        avg_percentages = df[quadrant_cols].mean().reset_index()
        avg_percentages.columns = ['Quadrant', 'Average Percentage']
        avg_percentages['Quadrant'] = ['Q1 (Top-Right)', 'Q2 (Top-Left)', 'Q3 (Bottom-Left)', 'Q4 (Bottom-Right)']
        
        table_md = avg_percentages.to_markdown(index=False)
        
        fig, ax = plt.subplots()
        ax.bar(avg_percentages['Quadrant'], avg_percentages['Average Percentage'])
        ax.set_ylabel('Average Percentage of Tiles Played (%)')
        ax.set_title('Average Quadrant Usage Across All Games')
        plt.savefig(quadrant_chart_filename)
        plt.close(fig)
        
        update_status(status_widget, "Quadrant analysis complete.")
        return {
            'quadrant_table': table_md,
            'quadrant_chart_path': quadrant_chart_filename
        }
    except Exception as e:
        update_status(status_widget, f"ERROR during quadrant analysis: {e}")
        return None




def generate_heatmap_analysis(status_widget):
    """
    Generates the square usage heatmap.
    
    Args:
        status_widget: The tkinter widget for status updates.

    Returns:
        A dictionary containing the heatmap image path for the template,
        or None if an error occurs.
    """
    import json
    import seaborn as sns
    import pandas as pd

    
    update_status(status_widget, "Generating square usage heatmap...")
    try:
        heatmap_filename = "square_usage_heatmap.png"
        square_counts_filename = "square_usage_counts.json"

        with open(square_counts_filename, 'r') as f:
            heatmap_data = json.load(f)
        
        counts = heatmap_data['square_counts']
        num_games = heatmap_data['games_processed']
        
        if num_games == 0:
            update_status(status_widget, "Warning: 0 games processed, cannot generate heatmap.")
            return None
            

        # Calculate percentages
        percentages = [[(count / num_games) * 100 for count in row] for row in counts]
        
        # Create custom labels for the axes
        row_labels = [str(i) for i in range(1, 16)] # '1' through '15'
        col_labels = list('ABCDEFGHIJKLMNO')       # 'A' through 'O'

        # Create a Pandas DataFrame with the data and custom labels
        df_heatmap = pd.DataFrame(percentages, index=row_labels, columns=col_labels)
        
        fig, ax = plt.subplots(figsize=(12, 10))
        # Pass the DataFrame to seaborn; it will use the index/columns for labels
        sns.heatmap(df_heatmap, annot=True, fmt=".1f", cmap="viridis", linewidths=.5, ax=ax)
        ax.set_title('Heatmap of Square Usage Frequency (%)')
        ax.set_xlabel('Column')
        ax.set_ylabel('Row')
        plt.savefig(heatmap_filename)
        plt.close(fig)
        
        update_status(status_widget, "Heatmap analysis complete.")
        return {'heatmap_path': heatmap_filename}
        
    except FileNotFoundError:
        update_status(status_widget, f"ERROR: Heatmap data file '{square_counts_filename}' not found.")
        update_status(status_widget, "Run 'Gather Stats' to generate it.")
        return None
    except Exception as e:
        update_status(status_widget, f"ERROR during heatmap analysis: {e}")
        return None





def generate_dominant_quadrant_analysis(df, status_widget):
    """
    Analyzes if dominating a quadrant correlates with average scores.
    
    Args:
        df (pd.DataFrame): The main dataframe with all game stats.
        status_widget: The tkinter widget for status updates.

    Returns:
        A dictionary containing the markdown table and chart path for the template,
        or None if an error occurs.
    """
    import numpy as np

    update_status(status_widget, "Analyzing scores by dominant quadrant...")
    try:
        chart_filename = "dominant_quadrant_scores_chart.png"
        quad_count_cols = ['q1_count', 'q2_count', 'q3_count', 'q4_count']

        # For each row (game), find the column name with the maximum count.
        # This determines the dominant quadrant for that game.
        df['dominant_quadrant'] = df[quad_count_cols].idxmax(axis=1)
        
        # Group by the new 'dominant_quadrant' column and calculate average scores
        grouped_scores = df.groupby('dominant_quadrant')[['p1_score', 'p2_score']].mean()
        grouped_scores.rename(columns={
            'p1_score': 'Average P1 Score',
            'p2_score': 'Average P2 Score'
        }, inplace=True)
        
        # --- Generate Table ---
        # Create a DataFrame for the table
        table_df = grouped_scores.reset_index()

        # 1. Rename the column header from 'dominant_quadrant'
        table_df.rename(columns={'dominant_quadrant': 'Dominant Quadrant'}, inplace=True)

        # 2. Create a mapping for the row values and apply it
        quadrant_name_map = {
            'q1_count': 'Q1',
            'q2_count': 'Q2',
            'q3_count': 'Q3',
            'q4_count': 'Q4'
        }
        table_df['Dominant Quadrant'] = table_df['Dominant Quadrant'].map(quadrant_name_map)

        # Convert the cleaned-up DataFrame to Markdown
        table_md = table_df.to_markdown(index=False)
        
        # --- Generate Grouped Bar Chart ---
        labels = [f"Q{i+1}" for i in range(4)] # Q1, Q2, Q3, Q4
        p1_scores = grouped_scores.loc[[f'q{i+1}_count' for i in range(4)], 'Average P1 Score']
        p2_scores = grouped_scores.loc[[f'q{i+1}_count' for i in range(4)], 'Average P2 Score']

        x = np.arange(len(labels))  # the label locations
        width = 0.35  # the width of the bars

        fig, ax = plt.subplots(figsize=(10, 6))
        rects1 = ax.bar(x - width/2, p1_scores, width, label='P1 Avg Score')
        rects2 = ax.bar(x + width/2, p2_scores, width, label='P2 Avg Score')

        # Add some text for labels, title and axes ticks
        ax.set_ylabel('Average Score')
        ax.set_title('Average Scores by Dominant Quadrant')
        ax.set_xticks(x)
        ax.set_xticklabels(labels)
        ax.legend()

        fig.tight_layout()
        plt.savefig(chart_filename)
        plt.close(fig)
        
        update_status(status_widget, "Dominant quadrant analysis complete.")
        return {
            'dominant_quad_table': table_md,
            'dominant_quad_chart_path': chart_filename
        }

    except Exception as e:
        update_status(status_widget, f"ERROR during dominant quadrant analysis: {e}")
        return None






def generate_opening_orientation_analysis(df, status_widget):
    """
    Analyzes quadrant usage based on the opening move's orientation.
    
    Args:
        df (pd.DataFrame): The main dataframe with all game stats.
        status_widget: The tkinter widget for status updates.

    Returns:
        A dictionary containing the markdown table for the template,
        or None if an error occurs.
    """
    update_status(status_widget, "Analyzing quadrant usage by opening orientation...")
    try:
        if 'opening_direction' not in df.columns:
            update_status(status_widget, "ERROR: 'opening_direction' column not found in CSV.")
            update_status(status_widget, "Please re-run 'Gather Stats' to add it.")
            return None

        quad_perc_cols = ['q1_perc', 'q2_perc', 'q3_perc', 'q4_perc']
        
        # Group by opening direction and calculate the mean of the percentage columns
        grouped_data = df.groupby('opening_direction')[quad_perc_cols].mean()

        # Clean up the table for presentation
        grouped_data.rename(columns={
            'q1_perc': 'Q1 %', 'q2_perc': 'Q2 %',
            'q3_perc': 'Q3 %', 'q4_perc': 'Q4 %'
        }, inplace=True)
        
        orientation_map = {'H': 'Horizontal', 'V': 'Vertical'}
        grouped_data.index = grouped_data.index.map(orientation_map)
        grouped_data.rename_axis('Opening Orientation', inplace=True)
        
        # Convert the final, styled DataFrame to a Markdown table
        table_md = grouped_data.reset_index().to_markdown(index=False, floatfmt=".2f")

        update_status(status_widget, "Opening orientation analysis complete.")
        return {'opening_orientation_table': table_md}

    except Exception as e:
        update_status(status_widget, f"ERROR during opening orientation analysis: {e}")
        return None





def generate_quadrant_control_analysis(df, status_widget):
    """
    Analyzes if concentrating play in a few quadrants correlates with winning.
    Uses the Herfindahl-Hirschman Index (HHI) as a measure of concentration.
    
    Args:
        df (pd.DataFrame): The main dataframe with all game stats.
        status_widget: The tkinter widget for status updates.

    Returns:
        A dictionary containing the markdown table and chart path for the template,
        or None if an error occurs.
    """


    import seaborn as sns

    
    update_status(status_widget, "Analyzing quadrant control strategy...")
    try:
        chart_filename = "quadrant_control_chart.png"
        quad_count_cols = ['q1_count', 'q2_count', 'q3_count', 'q4_count']

        # --- Calculate HHI for each game ---
        # HHI = sum of squared shares. Ranges from 0.25 (balanced) to 1.0 (concentrated).
        total_counts = df[quad_count_cols].sum(axis=1)
        # Avoid division by zero for games with no moves
        total_counts[total_counts == 0] = 1 
        
        shares = df[quad_count_cols].div(total_counts, axis=0)
        squared_shares = shares.pow(2)
        df['hhi_score'] = squared_shares.sum(axis=1)

        # --- Generate Scatter Plot ---
        df['score_differential'] = df['p1_score'] - df['p2_score']
        df['p1_win'] = df['score_differential'] > 0

        fig, ax = plt.subplots(figsize=(10, 6))
        sns.regplot(
            data=df,
            x='hhi_score',
            y='score_differential',
            ax=ax,
            scatter_kws={'alpha': 0.5},  # Make points semi-transparent
            line_kws={'color': 'red', 'linestyle': '--'} # Style the trend line
        )

        ax.set_title('Quadrant Concentration (HHI) vs. Score Differential')
        ax.set_xlabel('Concentration Score (HHI) -> More Concentrated')
        ax.set_ylabel('Final Score Differential (P1 - P2)')
        ax.grid(True)
        plt.savefig(chart_filename)
        plt.close(fig)

        # --- Generate Binned Table ---
        # Bin the HHI scores into categories
        bins = [0.24, 0.33, 0.5, 1.01]
        labels = ['Balanced (Spread Out)', 'Moderate Concentration', 'Highly Concentrated']
        df['concentration_strategy'] = pd.cut(df['hhi_score'], bins=bins, labels=labels)

        # Calculate P1 win percentage for each strategy
        win_rate_by_strategy = df.groupby('concentration_strategy', observed=False)['p1_win'].value_counts(normalize=True).unstack()
        # Ensure a 'True' column exists even if no wins in a category
        if True not in win_rate_by_strategy.columns:
            win_rate_by_strategy[True] = 0
        
        win_rate_by_strategy['P1 Win %'] = win_rate_by_strategy[True].fillna(0) * 100
        
        table_df = win_rate_by_strategy[['P1 Win %']].reset_index()
        table_df.rename(columns={'concentration_strategy': 'Quadrant Strategy'}, inplace=True)
        table_md = table_df.to_markdown(index=False, floatfmt=".2f")

        update_status(status_widget, "Quadrant control analysis complete.")
        return {
            'quad_control_table': table_md,
            'quad_control_chart_path': chart_filename
        }

    except Exception as e:
        update_status(status_widget, f"ERROR during quadrant control analysis: {e}")
        return None





def generate_comeback_analysis(status_widget):
    """
    Generates the comeback analysis table from its dedicated JSON file.
    
    Args:
        status_widget: The tkinter widget for status updates.

    Returns:
        A dictionary containing the markdown table for the template,
        or None if an error occurs.
    """
    import json
    
    update_status(status_widget, "Generating comeback strategy analysis...")
    try:
        results_filename = "comeback_analysis_results.json"
        with open(results_filename, 'r') as f:
            data = json.load(f)
        
        # Convert the loaded JSON data into a Pandas DataFrame for easy table creation
        df = pd.DataFrame(data)
        df.rename(columns={
            'strategy': 'Comeback Strategy',
            'attempts': 'Total Attempts',
            'wins': 'Total Wins',
            'win_rate_percent': 'Win Rate %'
        }, inplace=True)

        table_md = df.to_markdown(index=False, floatfmt=".2f")
        
        update_status(status_widget, "Comeback analysis complete.")
        return {'comeback_table': table_md}

    except FileNotFoundError:
        update_status(status_widget, f"ERROR: Comeback analysis file '{results_filename}' not found.")
        update_status(status_widget, "Please re-run 'Gather Stats' to generate it.")
        return None
    except Exception as e:
        update_status(status_widget, f"ERROR during comeback analysis: {e}")
        return None
    






def start_report_generation(status_widget):
    """Orchestrator function called by the 'Generate Report' button."""
    # This function can be expanded to take options for which report to generate
    generate_analysis_report(status_widget)



    


# =================================================================================================
# /////////////////////////////////////////////////////////////////////////////////////////////////
# --- MAIN EXECUTION ---
# /////////////////////////////////////////////////////////////////////////////////////////////////
# =================================================================================================

if __name__ == "__main__":
    setup_gui()
